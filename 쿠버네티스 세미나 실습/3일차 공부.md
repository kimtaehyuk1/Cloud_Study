## 네트워크를 위한 컨테이너 인프라 응용

쿠버네티스 환경 클러스터에서 파드(컨테이너의 집합이지만 원칙은 메인은 1개)배포되면 노드중에 골라져서 배포가 되어진다.

이러한 환경에서 쿠버네티스 네트워크 통신 유형을 살펴보면
1. 파드안에서의 컨테이너간 통신 : 파드의 특징은 하나의 NIC를 제공해서 안에 컨테이너들은 그 주소를 공유한다 =  하나의 IP를 쓴다. 그러면 컨테이너끼리
구분은 어떻게 하냐? 이들은 서비스 포트로 구분을 한다. 이 포트라는것이 IP에 의존하는데 IP가 다르면 동일 포트 가능하지만 IP가 동일할경우 같으면 포트 충돌 나니까 다르 포트로 구분

2. 파드 투 파드 통신 : 이들의 파드는 같은 노드가 아닌 다른 노드에 올라갈 수 있다. 기본적으로는 파드 투 파드 의 네트워킹 매커니즘 제공하지 않는데 그러면 컨테이너 엔진인
도커 엔진 네트워킹 기능에 의존해야되는데 도커라고 하는 놈은 기본적으로 docker0라는 스위치를 설치해서 서브넷도 동일하게 올라간다. 즉 동일한 서브넷 기반에 파드가 배포가 된다면
같은 서브넷 IP부여받을것이고 그러면 충돌이 날 것이다. 즉 다른 노드에 배포된 파드에 대해서는 서브넷도 분리도 해야되고 그러면 다른 서브넷으로 동작함으로 파드와파드간에 다른 서브넷을
갖는 IP로 올라올것이고 통신을 하려면 
1)라우팅 테이블을 서로 공유를 해서 통신 할 수 밖에 없다.(이때 BGP라는 프로토콜 이용해서 지원) 도 하고
2)Overlay 네트워크 방식을 이용해서 파드 투 파드 통신을 지원할 수도 있다. (IP-in-IP 방식도 마찬가지 이거 둘다 encapsulation 이용해서)
-> 이 부분은 외부의 네트워크 플러그인을 설치를 해야된다는 것이다.

**오늘은 파드와 내외부 서브스간 통신에 초점을 맞춘 네트워킹 방식 공부**
예를 들어 웹서비스는 외부의 인바운드를 받아서 처리하는 형태로 DB는 내부의 어떤 서비스들이 커넥트해서 서비스하는 내부 서비스로 처리하면 된다는 것이다. 따라서
이러한 내외부 통신 매커니즘이 필요하다. !! 컨테이너간,내부 서비스간 통신,외부 서비스간 통신은 쿠버네티스 자체적으로 매커니즘을 제공하고 있다. 

클러스터 환경을 쓴다는것 자체가 어느정도 고가용성 H.A를 기본적인 기능을 제공한다라는 것을 포함하고 있다. 결국 노드가 페일이 나든 파드가 잘못되든 쿠버네티스는
이러건 알아서 처리해주는데 그걸 컨트롤러가 해준다는 말이다. 또 파드는 커맨드를 가지고 배포할수 있지만 Yaml이라는 명세서를 가지고도 배포할수있다(그안에 다양한 속성 정의해서
배포가능) 또 어플리케이션을 파드단위로 파드안에 컨테이너 형태로 배포하게 되는데 이런 어플리케이션 서비스가 원할하게 동작 가능한지 파드 내에서 체크하는 매커니즘을
제공하고 있다.(파드 야물 명세서 안에서 정의한다) 또 파드가 프로비저닝이 되는 시간이 필요한데 이시간이 어플리케이션에 따라 다를수 있는데 무작정 파드가 생성되면서 IP가 부여되는데
IP부여 되었다해서 통신 요청하면 애플리케이션이 서비스가 올라오지 않은 상태면 fail발생 이렇듯 서비스 요청에 대해서 서비스 다운타임이 발생하지 않도록 내부적으로 Probe들을 define할수
있다. 즉 파드를 svc라는 리소스와 결합을 하여서 관리하게 된다(이놈이 LB역할)

또 이때 LB에서는 endpoint라는 리소스를 사용하고 있는데 파드에 대한 IP와 애플리케이션에 대한 서비스포트가 결합된 엔드포인트 갖는다. 그래서 안에 파드나 애플리케이션의 동작여부를
체크를 하는것이다(리눅스 명령이나 http프로토콜,TCP 이용해서). 문제가 있으면 제거할수 있는데 그것을 ReadinessProbe가 한다 또 LivenessProbe를 define할수 있는데 앞꺼와 유사하게 응답여부에
따라서 container restart하는(파드 재시작 아님) 매커니즘 제공 또 위의 두개를 무작정 간격을 가지고 retry 할것이 아니라 startupProbe를 이용해서 이것이 성공하기 까지 프로브들은 대기하고 이게 성공하면
그 이후에 체크에 들어간다.
-> 이러한 3가지 매커니즘을 가지고 파드안에 서비스가 정상적으로 동작여부 확인

- Qos(여기선 파드인데 cpu많이 잡아먹는 그러한 정도나 우선순위 떨어지는 파드 부터)수준에 따라 Kubelet(이놈은 각 자기 노드 모니터링함)에 의해 제거 
- (물리적인 CPU 보다 더 많은 파드들이 배포되는것 때문에) -> 리소스 부족으로 파드 제거된상황에서 컨트롤러 입장에서 이놈에 대한 서비스에 고가용성 지원하기위해 다시 스케쥴링 요청   

컨트롤러를 통해서 배포되어야 고가용성이 제공될수 있다.

다양한 컨트롤러들이 있는 것은 애플리케이션의 워크노드 패턴에 따라서 적절히 운영방식을 다르게 하고 싶어서 이다. 예를들어 배치성작업이 필요한 빅데이터를 처리하기 위해 데이터레이크에서
읽어서  ETL을 처리하고 저장소에 저장하려는 배치작업을 처리할라하면 이런거는 한번 완료가되면 계속해서 restart할 필요가 없는데, 그래서 이런한 워크노드에 유형에 따라 적절히 컨트롤러를
선택해서 쓴다.(방금껀 배포할때)

**즉 컨트롤러 선택에 따라 그놈이 파드를 관리하는 방식이 달라진다** (이 모든것이다 마스터 노드 4인방중 컨트롤러 매니저의 하나의 펑셩형태로 제어된다 이러한 정보는 다 etcd로)
### 1. DaemonSet
yaml명세서에서 kind:DaemonSet 또 파드를 찍어내기때문에 파드안에 어떤 컨테이너를 파드로 배포할지 spec에 명시
또 데몬셋으로 파드를 배포하면 각각 노드에 다 한개씩 파드를 만든다 즉 한개의 노드에 두개이상의 파드가 올라가지 않는다.
이렇게 하는 방식으로 시스템 에이전트 로그 분석하는 애들를 각각 노드에 1개씩 심을때 이 기반으로 배포한다. 또는 네트웍플러그인 깔때 등등..

### 2. Deployment Controller
워크노드 패턴으로는 stateless앱을 배포하기 위한 목적으로 이것 선택(여기서 stateless app의 특징은 트래픽이 막 몰렸을때 확장해서 트래픽을 처리할수 있게끔 디자인된 app이라 생각 대표
적으로 웹서비스들이 scale out 기존의 파드를 복제해서 물론 앞에LB둬야됨 대신 스케일 아웃된놈들은 하나의 볼륨을 공유한다. 좀더 어렵게 얘기해서
파드가 배포된 로컬 시스템이 있을텐데 그 로컬 스토리지에 저장하지 않고 stateless하게 외부저장소를 사용하도록 디자인된 설계 방식 그래서 얼마든지 파드복제가능한데
만약에 statefull이라면 즉 로컬에 저장하는 형태라면 만약 두번쨰 파드가 다른 노드에 있을수 있을것이고 그러면 기존의 다른노드에있는 파드에 있는 세션정보는 자동로그아웃된다.
**그래서 stateless방식은 scale out방식으로 확장해서 트래픽을 처리하도록 디자인된 방식이고 그놈 관리하는게 deployment이다**
또 이놈은 하위 컨트롤러로 replica set이라는 두고 이놈이 파드 배포하도록 한다. 중간에 하위를 두면 버젼관리하기가 쉬워진다.

!!!!!쿠버네티스는 데브옵스 플랫폼으로 개발자가 그들의 앱을 변경해서 적용해서 쓰는데 저장소에 v1이있고 v2를 적용할려고 하면 이미지만 바꿔주면 되는데 자동으로 
roling update형태로 트리거링이된다-> 이미지를 바꾸는 순간 새로운 replica set이 만들어지고 버전2로 파드가 만들어져서 서비스가된다.-> 그리고 버전1 파드는 삭제를 한다.
만약에 새로운 버전에 버그가 있으면 roll back(이전상태로 되돌리는)해야되는데 다시 v1로 롤백이 쉽게 이루어 진다. 또 여러 버전 믹스해서 동시에도 가능하면서 피드백을 통해
안전한 버전으로 서서히 진행해나간다.

자세한 내용은 강의보면서 yaml명세서로 따라오고 대충 중요부분 적어보면
(yaml명세서에서) Deployment에서의 selector가 이놈의 하위 리소스인 replica set의 lable 과 같아야지 둘이 연동되는것이고
rs의 하위는 template가 파드를 찍으니까 거기서 레이블 찾아보기. 반드시 파드부분의 labels에는 rs의 selector matchLabels가 가지고 있는거를 가지고 있어야한다.

-만약 StatefulSet(레플리카셋과 비슷한 얘인듯 앱배포방식에서 차이나는 놈, 디플로이와다르게 state app에서 씀) 에서 scale out으로 파드복제하면 이놈은 pv와 pvc매커니즘을 쓰는데
이것은 rs에서 파드복제시 볼륨공유하는것과는 다르게 즉 파드안에서 다이렉트로 볼륨을 define해서 사용하기 보다는 디커플링 메소드로
개발자가 파드명세서 쓸때 스토리지 요청을 pvc를 별도로 만들어서(걍 쉽게 파드와 스토리지까지 가기위한 길이라생각) 요청해야 된다.
명세서에서 일일이 다 명세안하고 추상화된 디커플링된 개발자는 다 속성이 맞춰진놈만 딱 쓸수있게 머 그런식이다.
어쩃든 statefulset에서 파드 복제시 스토리지가 같이 복제된다.

## Service
LB같은 엔드포인트 역활을 하는데 Blue/Green 배포라해서 미리 엔드포인트를 옮길 버전을 만들고 나서(리소스2배쓰는거) 그후에 서비스가 엔드포인트를 바꾼다.
그방식이 상위객체는 selector라는 속성이있고 그놈이 관리하게끔하려고 label을 지정한다. 그래서 v1의 label로 갖고 있다가 selector값만 v2로 바꾸면 된다.그러면
엔드포인트가 기존꺼 지워지고 다시 생성이되는거다.

파드는 사설 ip갖는데 외부에서 접근하려면 공인 ip를 가져야 한다. LB를 통해서 접근해야된다. 이때 사용하는게 Service 리소스이다. L4스위치라고 생각하기
이놈 생성 하면 고정 ip를 생성 selector의 내용을 밑의 파드가 가지는 label로 설정해준다. 이런식으로 엔드포인트가 생성이된다. 파드 갯수만큼 엔트포인트가 만들어진다.
이 서비스를 사용하는 이유는 고가용성(한 파드가 삭제되면 컨트롤러가 다시 파드 생성)을 위함. 파드가 삭제되면 ip가 틀어지고 이름도 바뀌어버린다. 그래서 고정적인 놈이 필요한것이고
그것이 LB이자 Service를 사용하는 이유이다. **간단하게 deploy를 통해 rs를 통해 배포된 파드들이 있을텐데(여러노드에) 그것과 연결되는 서비스인 LB를 생성하는것이 일반적인 것
deployment와 rs가 담당하는것은 파드에 대한 고가용성이고, LB는 트래픽 담당이다.**
맞는진 모르겠지만 LB로 노드를 고르고 고른노드에서 서비스통해 맞는 파드에 트래픽 전달 뭐 이런식으로도 생각해봄...이때 노드마다 각각 공인ip가 있는 서비스가 있다...뭐 이런식
로드밸런서에 사실 밑에서볼 노드포트와 클러스트아이피 다 포함한다! 그니까 서비스가 LB라 하면 트래픽에 맞는 파드까지 척척감
이때 워커노드의 kube-proxy가 관여되어서 트래픽을 컨트럴한다.

-Service Types 
1. ClusterIP
이것이 기본인데 iptable로서 이놈 거치면 외부에서 들어오는놈들을 이놈이 파드에 사설 ip 뿌려서 연결해준다.( 여기에 --external ip 지정할수 있는데 그래야 지정된 ip로 외부의 인바운드 요청을 받아서 내부 서비스로 포워딩 한다.)
웹서비스 같은 프론트엔드 서비스는 ClusterIp타입인데, external ip로 특정 인바운드 받아서 이놈은 하나!!의 노드가 트래픽받아서 파드가 있는 시스템쪽으로 
2. NodePort
이놈은 노드가 여러개가 있을텐데 노드포트를 하나 맵핑을 시켜서 노드의 포트로 외부에서 인식시킨후 해당 서비스(ClusterIP)로 포워딩해서 포워딩함
이걸로 만들면 모든 멤버 노드들이 워커노드들이 동일한 포트 다 열려서 외부의 인바운드가 분산되어 받을 수 있다.

실습을 보니까 디플로를 만들어놓고 그거에 맞는 서비스를 연결시키는 느낌인듯.
클러스터 ip는 내부끼리 파드투파드 통신할때(파드투파드 경우는 백엔드이미지와 프론트엔드 이미지간)
