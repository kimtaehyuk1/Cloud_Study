## 네트워크를 위한 컨테이너 인프라 응용

쿠버네티스 환경 클러스터에서 파드(컨테이너의 집합이지만 원칙은 메인은 1개)배포되면 노드중에 골라져서 배포가 되어진다.

이러한 환경에서 쿠버네티스 네트워크 통신 유형을 살펴보면
1. 파드안에서의 컨테이너간 통신 : 파드의 특징은 하나의 NIC를 제공해서 안에 컨테이너들은 그 주소를 공유한다 =  하나의 IP를 쓴다. 그러면 컨테이너끼리
구분은 어떻게 하냐? 이들은 서비스 포트로 구분을 한다. 이 포트라는것이 IP에 의존하는데 IP가 다르면 동일 포트 가능하지만 IP가 동일할경우 같으면 포트 충돌 나니까 다르 포트로 구분

2. 파드 투 파드 통신 : 이들의 파드는 같은 노드가 아닌 다른 노드에 올라갈 수 있다. 기본적으로는 파드 투 파드 의 네트워킹 매커니즘 제공하지 않는데 그러면 컨테이너 엔진인
도커 엔진 네트워킹 기능에 의존해야되는데 도커라고 하는 놈은 기본적으로 docker0라는 스위치를 설치해서 서브넷도 동일하게 올라간다. 즉 동일한 서브넷 기반에 파드가 배포가 된다면
같은 서브넷 IP부여받을것이고 그러면 충돌이 날 것이다. 즉 다른 노드에 배포된 파드에 대해서는 서브넷도 분리도 해야되고 그러면 다른 서브넷으로 동작함으로 파드와파드간에 다른 서브넷을
갖는 IP로 올라올것이고 통신을 하려면 
1)라우팅 테이블을 서로 공유를 해서 통신 할 수 밖에 없다.(이때 BGP라는 프로토콜 이용해서 지원) 도 하고
2)Overlay 네트워크 방식을 이용해서 파드 투 파드 통신을 지원할 수도 있다. (IP-in-IP 방식도 마찬가지 이거 둘다 encapsulation 이용해서)
-> 이 부분은 외부의 네트워크 플러그인을 설치를 해야된다는 것이다.

**오늘은 파드와 내외부 서브스간 통신에 초점을 맞춘 네트워킹 방식 공부**
예를 들어 웹서비스는 외부의 인바운드를 받아서 처리하는 형태로 DB는 내부의 어떤 서비스들이 커넥트해서 서비스하는 내부 서비스로 처리하면 된다는 것이다. 따라서
이러한 내외부 통신 매커니즘이 필요하다. !! 컨테이너간,내부 서비스간 통신,외부 서비스간 통신은 쿠버네티스 자체적으로 매커니즘을 제공하고 있다. 

클러스터 환경을 쓴다는것 자체가 어느정도 고가용성 H.A를 기본적인 기능을 제공한다라는 것을 포함하고 있다. 결국 노드가 페일이 나든 파드가 잘못되든 쿠버네티스는
이러건 알아서 처리해주는데 그걸 컨트롤러가 해준다는 말이다. 또 파드는 커맨드를 가지고 배포할수 있지만 Yaml이라는 명세서를 가지고도 배포할수있다(그안에 다양한 속성 정의해서
배포가능) 또 어플리케이션을 파드단위로 파드안에 컨테이너 형태로 배포하게 되는데 이런 어플리케이션 서비스가 원할하게 동작 가능한지 파드 내에서 체크하는 매커니즘을
제공하고 있다.(파드 야물 명세서 안에서 정의한다) 또 파드가 프로비저닝이 되는 시간이 필요한데 이시간이 어플리케이션에 따라 다를수 있는데 무작정 파드가 생성되면서 IP가 부여되는데
IP부여 되었다해서 통신 요청하면 애플리케이션이 서비스가 올라오지 않은 상태면 fail발생 이렇듯 서비스 요청에 대해서 서비스 다운타임이 발생하지 않도록 내부적으로 Probe들을 define할수
있다. 즉 파드를 svc라는 리소스와 결합을 하여서 관리하게 된다(이놈이 LB역할)

또 이때 LB에서는 endpoint라는 리소스를 사용하고 있는데 파드에 대한 IP와 애플리케이션에 대한 서비스포트가 결합된 엔드포인트 갖는다. 그래서 안에 파드나 애플리케이션의 동작여부를
체크를 하는것이다(리눅스 명령이나 http프로토콜,TCP 이용해서). 문제가 있으면 제거할수 있는데 그것을 ReadinessProbe가 한다 또 LivenessProbe를 define할수 있는데 앞꺼와 유사하게 응답여부에
따라서 container restart하는(파드 재시작 아님) 매커니즘 제공 또 위의 두개를 무작정 간격을 가지고 retry 할것이 아니라 startupProbe를 이용해서 이것이 성공하기 까지 프로브들은 대기하고 이게 성공하면
그 이후에 체크에 들어간다.
-> 이러한 3가지 매커니즘을 가지고 파드안에 서비스가 정상적으로 동작여부 확인

- Qos(여기선 파드인데 cpu많이 잡아먹는 그러한 정도나 우선순위 떨어지는 파드 부터)수준에 따라 Kubelet(이놈은 각 자기 노드 모니터링함)에 의해 제거 
- (물리적인 CPU 보다 더 많은 파드들이 배포되는것 때문에) -> 리소스 부족으로 파드 제거된상황에서 컨트롤러 입장에서 이놈에 대한 서비스에 고가용성 지원하기위해 다시 스케쥴링 요청   

컨트롤러를 통해서 배포되어야 고가용성이 제공될수 있다.

다양한 컨트롤러들이 있는 것은 애플리케이션의 워크노드 패턴에 따라서 적절히 운영방식을 다르게 하고 싶어서 이다. 예를들어 배치성작업이 필요한 빅데이터를 처리하기 위해 데이터레이크에서
읽어서  ETL을 처리하고 저장소에 저장하려는 배치작업을 처리할라하면 이런거는 한번 완료가되면 계속해서 restart할 필요가 없는데, 그래서 이런한 워크노드에 유형에 따라 적절히 컨트롤러를
선택해서 쓴다.(방금껀 배포할때)

**즉 컨트롤러 선택에 따라 그놈이 파드를 관리하는 방식이 달라진다** (이 모든것이다 마스터 노드 4인방중 컨트롤러 매니저의 하나의 펑셩형태로 제어된다 이러한 정보는 다 etcd로)
### 1. DaemonSet
yaml명세서에서 kind:DaemonSet 또 파드를 찍어내기때문에 파드안에 어떤 컨테이너를 파드로 배포할지 spec에 명시
또 데몬셋으로 파드를 배포하면 각각 노드에 다 한개씩 파드를 만든다 즉 한개의 노드에 두개이상의 파드가 올라가지 않는다.
이렇게 하는 방식으로 시스템 에이전트 로그 분석하는 애들를 각각 노드에 1개씩 심을때 이 기반으로 배포한다. 또는 네트웍플러그인 깔때 등등..



